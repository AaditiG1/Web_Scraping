{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "524a23ed-b682-41b9-beef-d25829640f65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\users\\asus1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.34.2)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\asus1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.13.3)\n",
      "Requirement already satisfied: pandas in c:\\users\\asus1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: fastapi in c:\\users\\asus1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.116.1)\n",
      "Requirement already satisfied: uvicorn in c:\\users\\asus1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.35.0)\n",
      "Requirement already satisfied: requests in c:\\users\\asus1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.32.3)\n",
      "Requirement already satisfied: webdriver-manager in c:\\users\\asus1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.0.2)\n",
      "Requirement already satisfied: urllib3~=2.5.0 in c:\\users\\asus1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from urllib3[socks]~=2.5.0->selenium) (2.5.0)\n",
      "Requirement already satisfied: trio~=0.30.0 in c:\\users\\asus1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from selenium) (0.30.0)\n",
      "Requirement already satisfied: trio-websocket~=0.12.2 in c:\\users\\asus1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from selenium) (0.12.2)\n",
      "Requirement already satisfied: certifi>=2025.6.15 in c:\\users\\asus1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from selenium) (2025.7.14)\n",
      "Requirement already satisfied: typing_extensions~=4.14.0 in c:\\users\\asus1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from selenium) (4.14.1)\n",
      "Requirement already satisfied: websocket-client~=1.8.0 in c:\\users\\asus1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from selenium) (1.8.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\asus1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from beautifulsoup4) (2.6)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\asus1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2.2.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\asus1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\asus1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\asus1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: starlette<0.48.0,>=0.40.0 in c:\\users\\asus1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from fastapi) (0.47.2)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in c:\\users\\asus1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from fastapi) (2.11.7)\n",
      "Requirement already satisfied: click>=7.0 in c:\\users\\asus1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from uvicorn) (8.2.1)\n",
      "Requirement already satisfied: h11>=0.8 in c:\\users\\asus1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from uvicorn) (0.14.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\asus1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\asus1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\asus1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from webdriver-manager) (1.1.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\asus1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from webdriver-manager) (24.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\asus1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from click>=7.0->uvicorn) (0.4.6)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\asus1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\asus1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\asus1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.4.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\asus1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: anyio<5,>=3.6.2 in c:\\users\\asus1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from starlette<0.48.0,>=0.40.0->fastapi) (4.8.0)\n",
      "Requirement already satisfied: attrs>=23.2.0 in c:\\users\\asus1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from trio~=0.30.0->selenium) (25.1.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\asus1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from trio~=0.30.0->selenium) (2.4.0)\n",
      "Requirement already satisfied: outcome in c:\\users\\asus1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from trio~=0.30.0->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\users\\asus1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from trio~=0.30.0->selenium) (1.3.1)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\asus1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from trio~=0.30.0->selenium) (1.17.1)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\asus1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from trio-websocket~=0.12.2->selenium) (1.2.0)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\asus1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from urllib3[socks]~=2.5.0->selenium) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\asus1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from cffi>=1.14->trio~=0.30.0->selenium) (2.22)\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "from urllib.parse import urljoin\n",
    "from fastapi import FastAPI, HTTPException, Query\n",
    "from pydantic import BaseModel\n",
    "import uvicorn\n",
    "import asyncio\n",
    "import sys\n",
    "import json\n",
    "import re\n",
    "!pip install selenium beautifulsoup4 pandas fastapi uvicorn requests webdriver-manager\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException\n",
    "from typing import List, Optional\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9f552aa-4b61-423a-8e24-c591eb17b926",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS1\\AppData\\Local\\Temp\\ipykernel_3640\\1628708305.py:392: DeprecationWarning: \n",
      "        on_event is deprecated, use lifespan event handlers instead.\n",
      "\n",
      "        Read more about it in the\n",
      "        [FastAPI docs for Lifespan Events](https://fastapi.tiangolo.com/advanced/events/).\n",
      "        \n",
      "  @app.on_event(\"startup\")\n",
      "C:\\Users\\ASUS1\\AppData\\Local\\Temp\\ipykernel_3640\\1628708305.py:398: DeprecationWarning: \n",
      "        on_event is deprecated, use lifespan event handlers instead.\n",
      "\n",
      "        Read more about it in the\n",
      "        [FastAPI docs for Lifespan Events](https://fastapi.tiangolo.com/advanced/events/).\n",
      "        \n",
      "  @app.on_event(\"shutdown\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selenium driver initialized successfully\n",
      "Daraz Nepal Product Scraper (CLI Mode)\n",
      "========================================\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter category (e.g., mobile-cases-covers):  mobile-phone-cases\n",
      "Enter start page (default 1):  1\n",
      "Enter end page (default 1):  2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scraping mobile-phone-cases from page 1 to 2...\n",
      "Scraping page 1: https://www.daraz.com.np/mobile-phone-cases/?page=1\n",
      "Loading page with Selenium: https://www.daraz.com.np/mobile-phone-cases/?page=1\n",
      "Timeout waiting for products, trying with current content...\n",
      "No products found on page 1\n",
      "Available classes: [['daraz']]\n",
      "Scraping page 2: https://www.daraz.com.np/mobile-phone-cases/?page=2\n",
      "Loading page with Selenium: https://www.daraz.com.np/mobile-phone-cases/?page=2\n",
      "Timeout waiting for products, trying with current content...\n",
      "No products found on page 2\n",
      "Available classes: [['daraz']]\n",
      " No products were scraped.\n"
     ]
    }
   ],
   "source": [
    "# Pydantic models for API\n",
    "class ProductModel(BaseModel):\n",
    "    title: str\n",
    "    price: str\n",
    "    original_price: Optional[str] = None\n",
    "    discount: Optional[str] = None\n",
    "    rating: str\n",
    "    reviews: str\n",
    "    image_url: str\n",
    "    product_url: str\n",
    "    category: str\n",
    "    page: int\n",
    "\n",
    "class ScrapeRequest(BaseModel):\n",
    "    category: str\n",
    "    start_page: int = 1\n",
    "    end_page: int = 1\n",
    "    use_selenium: bool = True\n",
    "\n",
    "class ScrapeResponse(BaseModel):\n",
    "    status: str\n",
    "    count: int\n",
    "    products: List[ProductModel]\n",
    "    message: Optional[str] = None\n",
    "\n",
    "class DarazScraper:\n",
    "    def __init__(self, use_selenium=True):\n",
    "        self.base_url = \"https://www.daraz.com.np\"\n",
    "        self.use_selenium = use_selenium\n",
    "        self.driver = None\n",
    "        self.headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36',\n",
    "            'Accept-Language': 'en-US,en;q=0.5',\n",
    "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "            'Accept-Encoding': 'gzip, deflate, br',\n",
    "            'Connection': 'keep-alive',\n",
    "            'Upgrade-Insecure-Requests': '1'\n",
    "        }\n",
    "        \n",
    "        if self.use_selenium:\n",
    "            self.setup_driver()\n",
    "    \n",
    "    def setup_driver(self):\n",
    "        \"\"\"Setup Chrome driver with options for web scraping\"\"\"\n",
    "        chrome_options = Options()\n",
    "        chrome_options.add_argument('--headless')\n",
    "        chrome_options.add_argument('--no-sandbox')\n",
    "        chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "        chrome_options.add_argument('--disable-gpu')\n",
    "        chrome_options.add_argument('--window-size=1920,1080')\n",
    "        chrome_options.add_argument(f'--user-agent={self.headers[\"User-Agent\"]}')\n",
    "        chrome_options.add_argument('--disable-blink-features=AutomationControlled')\n",
    "        chrome_options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "        chrome_options.add_experimental_option('useAutomationExtension', False)\n",
    "        \n",
    "        try:\n",
    "            self.driver = webdriver.Chrome(options=chrome_options)\n",
    "            self.driver.execute_script(\"Object.defineProperty(navigator, 'webdriver', {get: () => undefined})\")\n",
    "            self.driver.implicitly_wait(10)\n",
    "            print(\"Selenium driver initialized successfully\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error setting up Chrome driver: {e}\")\n",
    "            print(\"Falling back to requests method...\")\n",
    "            self.use_selenium = False\n",
    "    \n",
    "    def close_driver(self):\n",
    "        \"\"\"Close the Selenium driver\"\"\"\n",
    "        if self.driver:\n",
    "            self.driver.quit()\n",
    "        \n",
    "    def get_full_url(self, path):\n",
    "        \"\"\"Convert relative URL to absolute URL\"\"\"\n",
    "        if not path or path.startswith('http'):\n",
    "            return path\n",
    "        return urljoin(self.base_url, path)\n",
    "\n",
    "    def scrape_with_selenium(self, url):\n",
    "        \"\"\"Scrape using Selenium for dynamic content\"\"\"\n",
    "        try:\n",
    "            print(f\"Loading page with Selenium: {url}\")\n",
    "            self.driver.get(url)\n",
    "            \n",
    "            # Wait for products to load\n",
    "            try:\n",
    "                WebDriverWait(self.driver, 20).until(\n",
    "                    EC.any_of(\n",
    "                        EC.presence_of_element_located((By.CSS_SELECTOR, '[data-qa-locator=\"product-item\"]')),\n",
    "                        EC.presence_of_element_located((By.CSS_SELECTOR, '.gridItem--Yd0sa')),\n",
    "                        EC.presence_of_element_located((By.CSS_SELECTOR, '[data-tracking=\"product-card\"]')),\n",
    "                        EC.presence_of_element_located((By.CSS_SELECTOR, '.search-card-item'))\n",
    "                    )\n",
    "                )\n",
    "                print(\"Products loaded successfully\")\n",
    "            except TimeoutException:\n",
    "                print(\"Timeout waiting for products, trying with current content...\")\n",
    "            \n",
    "            # Scroll to load more products\n",
    "            self.driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            time.sleep(3)\n",
    "            \n",
    "            html = self.driver.page_source\n",
    "            return BeautifulSoup(html, 'html.parser')\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error with Selenium scraping: {e}\")\n",
    "            if self.driver:\n",
    "                return BeautifulSoup(self.driver.page_source, 'html.parser')\n",
    "            return None\n",
    "\n",
    "    def scrape_with_requests(self, url):\n",
    "        \"\"\"Fallback method using requests\"\"\"\n",
    "        try:\n",
    "            time.sleep(random.uniform(2, 4))\n",
    "            response = requests.get(url, headers=self.headers, timeout=15)\n",
    "            response.raise_for_status()\n",
    "            return BeautifulSoup(response.text, 'html.parser')\n",
    "        except Exception as e:\n",
    "            print(f\"Error with requests: {e}\")\n",
    "            return None\n",
    "\n",
    "    def extract_product_data(self, product_element):\n",
    "        \"\"\"Extract product data from a product element\"\"\"\n",
    "        try:\n",
    "            # Multiple selectors for different page layouts\n",
    "            title_selectors = [\n",
    "                '.title--wFj93 a',\n",
    "                '.title a', \n",
    "                '[data-qa-locator=\"product-item\"] .c16H9d a',\n",
    "                '.c16H9d a',\n",
    "                'a[title]',\n",
    "                'h3 a'\n",
    "            ]\n",
    "            \n",
    "            # Enhanced price extraction combining both approaches\n",
    "            price_selectors = [\n",
    "                '.price--NVB62',\n",
    "                '.price span',\n",
    "                '.c13VH6',\n",
    "                '.current-price',\n",
    "                '[data-qa-locator=\"product-price\"]'\n",
    "            ]\n",
    "            \n",
    "            # Enhanced original price selectors\n",
    "            original_price_selectors = [\n",
    "                '.origPrice--AoCxF',\n",
    "                '.original-price',\n",
    "                '.c1hkC1'\n",
    "            ]\n",
    "            \n",
    "            rating_selectors = [\n",
    "                '.rating--ZI3Ol',\n",
    "                '.rating',\n",
    "                '.c6LcCO'\n",
    "            ]\n",
    "            \n",
    "            reviews_selectors = [\n",
    "                '.rate--DCc0D',\n",
    "                '.rate',\n",
    "                '.c6LcCO + span'\n",
    "            ]\n",
    "            \n",
    "            image_selectors = [\n",
    "                '.image--WOyuZ img',\n",
    "                '.image img',\n",
    "                'img[data-qa-locator=\"product-image\"]',\n",
    "                'img'\n",
    "            ]\n",
    "            \n",
    "            # Extract title and URL\n",
    "            title_elem = None\n",
    "            product_url = \"\"\n",
    "            for selector in title_selectors:\n",
    "                title_elem = product_element.select_one(selector)\n",
    "                if title_elem:\n",
    "                    break\n",
    "            \n",
    "            if title_elem:\n",
    "                relative_url = title_elem.get('href', '')\n",
    "                product_url = self.get_full_url(relative_url)\n",
    "            \n",
    "            # Enhanced price extraction - combining both approaches\n",
    "            price_elem = None\n",
    "            price = \"N/A\"\n",
    "            \n",
    "            # First try with specific selectors\n",
    "            for selector in price_selectors:\n",
    "                price_elem = product_element.select_one(selector)\n",
    "                if price_elem:\n",
    "                    price = price_elem.get_text(strip=True)\n",
    "                    break\n",
    "            \n",
    "            # If specific selectors failed, use pattern matching approach\n",
    "            if price == \"N/A\" or not price_elem:\n",
    "                for tag in product_element.find_all(['span', 'div', 'p']):\n",
    "                    text = tag.get_text(strip=True)\n",
    "                    # Look for Rs, ₨, or number patterns that look like prices\n",
    "                    if re.search(r'Rs\\.?\\s*\\d+|₨\\s*\\d+|\\d{2,}(?:,\\d{3})*(?:\\.\\d{2})?', text):\n",
    "                        # Additional validation to avoid false positives\n",
    "                        if len(text) < 50 and not any(word in text.lower() for word in ['review', 'rating', 'sold', 'item']):\n",
    "                            price_elem = tag\n",
    "                            price = text\n",
    "                            break\n",
    "            \n",
    "            # Enhanced original price extraction\n",
    "            original_price_elem = None\n",
    "            original_price = None\n",
    "            \n",
    "            # First try with specific selectors\n",
    "            for selector in original_price_selectors:\n",
    "                original_price_elem = product_element.select_one(selector)\n",
    "                if original_price_elem:\n",
    "                    original_price = original_price_elem.get_text(strip=True)\n",
    "                    break\n",
    "            \n",
    "            # If specific selectors failed, use style-based approach\n",
    "            if not original_price_elem:\n",
    "                for tag in product_element.find_all(['span', 'div', 'p']):\n",
    "                    style = tag.get('style', '')\n",
    "                    classes = ' '.join(tag.get('class', []))\n",
    "                    text = tag.get_text(strip=True)\n",
    "                    \n",
    "                    # Check for strike-through styling or classes\n",
    "                    if (('line-through' in style or 'text-decoration-line: line-through' in style or \n",
    "                         any(keyword in classes.lower() for keyword in ['strike', 'original', 'old-price', 'crossed'])) and\n",
    "                        re.search(r'Rs\\.?\\s*\\d+|₨\\s*\\d+|\\d{2,}(?:,\\d{3})*(?:\\.\\d{2})?', text)):\n",
    "                        original_price_elem = tag\n",
    "                        original_price = text\n",
    "                        break\n",
    "            \n",
    "            # Enhanced rating extraction\n",
    "            rating_elem = None\n",
    "            rating = \"No rating\"\n",
    "            \n",
    "            # First try with specific selectors\n",
    "            for selector in rating_selectors:\n",
    "                rating_elem = product_element.select_one(selector)\n",
    "                if rating_elem:\n",
    "                    rating = rating_elem.get('aria-label', rating_elem.get_text(strip=True))\n",
    "                    break\n",
    "            \n",
    "            # If specific selectors failed, use pattern matching for ratings\n",
    "            if rating == \"No rating\":\n",
    "                for tag in product_element.find_all(['span', 'div', 'p']):\n",
    "                    text = tag.get_text(strip=True)\n",
    "                    # Look for rating patterns like \"4.5\", \"4 stars\", etc.\n",
    "                    if re.search(r'\\d\\.\\d|\\d+\\s*star|\\d+/5|★', text.lower()):\n",
    "                        # Additional validation to avoid false positives\n",
    "                        if len(text) < 30 and not any(word in text.lower() for word in ['review', 'sold', 'item', 'price']):\n",
    "                            rating = text\n",
    "                            break\n",
    "                            \n",
    "            \n",
    "            # Enhanced reviews count extraction\n",
    "            reviews_elem = None\n",
    "            reviews = \"0\"\n",
    "            \n",
    "            # First try with specific selectors\n",
    "            for selector in reviews_selectors:\n",
    "                reviews_elem = product_element.select_one(selector)\n",
    "                if reviews_elem:\n",
    "                    reviews = reviews_elem.get_text(strip=True)\n",
    "                    break\n",
    "            \n",
    "            # If specific selectors failed, use pattern matching for review counts\n",
    "            if reviews == \"0\":\n",
    "                for tag in product_element.find_all(['span', 'div', 'p']):\n",
    "                    text = tag.get_text(strip=True)\n",
    "                    # Look for review patterns like \"(123)\", \"123 reviews\", \"Sold 456\", etc.\n",
    "                    if re.search(r'\\(\\d+\\)|\\d+\\s*review|\\d+\\s*sold|\\d+\\s*rating', text.lower()):\n",
    "                        # Additional validation\n",
    "                        if len(text) < 50 and not any(word in text.lower() for word in ['price', 'discount', 'off']):\n",
    "                            reviews = text\n",
    "                            break\n",
    "            \n",
    "            # Extract image\n",
    "            image_elem = None\n",
    "            for selector in image_selectors:\n",
    "                image_elem = product_element.select_one(selector)\n",
    "                if image_elem and image_elem.get('src'):\n",
    "                    break\n",
    "            \n",
    "            # Enhanced discount calculation\n",
    "            discount = None\n",
    "            if price != \"N/A\" and original_price:\n",
    "                try:\n",
    "                    # Clean price strings and convert to float\n",
    "                    current_price = float(re.sub(r'[^\\d.]', '', price.replace(',', '')))\n",
    "                    orig_price = float(re.sub(r'[^\\d.]', '', original_price.replace(',', '')))\n",
    "                    if orig_price > current_price > 0:\n",
    "                        discount = f\"{int(((orig_price - current_price) / orig_price) * 100)}% off\"\n",
    "                except ValueError as e:\n",
    "                    print(f\"Error calculating discount: {e}\")\n",
    "                    pass\n",
    "            \n",
    "            # If discount not calculated, try to find existing discount text\n",
    "            if not discount:\n",
    "                for tag in product_element.find_all(['span', 'div', 'p']):\n",
    "                    text = tag.get_text(strip=True)\n",
    "                    # Look for discount patterns like \"50% off\", \"-30%\", \"Save 40%\"\n",
    "                    if re.search(r'\\d+%\\s*off|\\-\\d+%|save\\s*\\d+%', text.lower()):\n",
    "                        if len(text) < 20:  # Keep it short to avoid false positives\n",
    "                            discount = text\n",
    "                            break\n",
    "            \n",
    "            return {\n",
    "                'title': title_elem.get_text(strip=True) if title_elem else \"N/A\",\n",
    "                'price': price,\n",
    "                'original_price': original_price,\n",
    "                'discount': discount,\n",
    "                'rating': rating,\n",
    "                'reviews': reviews,\n",
    "                'image_url': self.get_full_url(image_elem.get('src', '')) if image_elem else \"N/A\",\n",
    "                'product_url': product_url\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting product data: {e}\")\n",
    "            return None\n",
    "\n",
    "    def scrape_category(self, category, start_page=1, end_page=1):\n",
    "        \"\"\"Scrape products from a category\"\"\"\n",
    "        all_products = []\n",
    "        \n",
    "        for page in range(start_page, end_page + 1):\n",
    "            url = f\"{self.base_url}/{category}/?page={page}\"\n",
    "            print(f\"Scraping page {page}: {url}\")\n",
    "            \n",
    "            try:\n",
    "                # Choose scraping method\n",
    "                if self.use_selenium and self.driver:\n",
    "                    soup = self.scrape_with_selenium(url)\n",
    "                else:\n",
    "                    soup = self.scrape_with_requests(url)\n",
    "                \n",
    "                if not soup:\n",
    "                    print(f\"Failed to get content from page {page}\")\n",
    "                    continue\n",
    "                \n",
    "                # Multiple selectors for product containers\n",
    "                product_selectors = [\n",
    "                    '[data-qa-locator=\"product-item\"]',\n",
    "                    '.gridItem--Yd0sa',\n",
    "                    '[data-tracking=\"product-card\"]',\n",
    "                    '.search-card-item',\n",
    "                    '.c2prKC'\n",
    "                ]\n",
    "                \n",
    "                products = []\n",
    "                for selector in product_selectors:\n",
    "                    products = soup.select(selector)\n",
    "                    if products:\n",
    "                        print(f\"Found {len(products)} products using selector: {selector}\")\n",
    "                        break\n",
    "                \n",
    "                if not products:\n",
    "                    print(f\"No products found on page {page}\")\n",
    "                    print(\"Available classes:\", [elem.get('class') for elem in soup.find_all()[:10] if elem.get('class')])\n",
    "                    continue\n",
    "                \n",
    "                page_products = 0\n",
    "                for product in products:\n",
    "                    product_data = self.extract_product_data(product)\n",
    "                    if product_data and product_data['title'] != \"N/A\":\n",
    "                        product_data.update({\n",
    "                            'category': category,\n",
    "                            'page': page\n",
    "                        })\n",
    "                        all_products.append(product_data)\n",
    "                        page_products += 1\n",
    "                \n",
    "                print(f\"Successfully extracted {page_products} products from page {page}\")\n",
    "                \n",
    "                # Add delay between pages\n",
    "                if page < end_page:\n",
    "                    time.sleep(random.uniform(3, 6))\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error scraping page {page}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        return all_products\n",
    "\n",
    "# Initialize FastAPI app\n",
    "app = FastAPI(\n",
    "    title=\"Daraz Nepal Product Scraper API\",\n",
    "    description=\"API for scraping product data from Daraz Nepal\",\n",
    "    version=\"2.0.0\"\n",
    ")\n",
    "\n",
    "# Global scraper instance\n",
    "scraper_instance = None\n",
    "\n",
    "@app.on_event(\"startup\")\n",
    "async def startup_event():\n",
    "    \"\"\"Initialize scraper on startup\"\"\"\n",
    "    global scraper_instance\n",
    "    scraper_instance = DarazScraper(use_selenium=True)\n",
    "\n",
    "@app.on_event(\"shutdown\")\n",
    "async def shutdown_event():\n",
    "    \"\"\"Cleanup on shutdown\"\"\"\n",
    "    global scraper_instance\n",
    "    if scraper_instance:\n",
    "        scraper_instance.close_driver()\n",
    "\n",
    "@app.get(\"/\")\n",
    "async def root():\n",
    "    return {\n",
    "        \"message\": \"Daraz Nepal Product Scraper API\",\n",
    "        \"version\": \"2.0.0\",\n",
    "        \"endpoints\": {\n",
    "            \"scrape\": \"/scrape\",\n",
    "            \"categories\": \"/categories\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "@app.post(\"/scrape\", response_model=ScrapeResponse)\n",
    "async def scrape_products(request: ScrapeRequest):\n",
    "    \"\"\"Scrape products from a category\"\"\"\n",
    "    global scraper_instance\n",
    "    \n",
    "    try:\n",
    "        if not scraper_instance:\n",
    "            scraper_instance = DarazScraper(use_selenium=request.use_selenium)\n",
    "        \n",
    "        print(f\"Starting scrape for category: {request.category}\")\n",
    "        products = scraper_instance.scrape_category(\n",
    "            request.category, \n",
    "            request.start_page, \n",
    "            request.end_page\n",
    "        )\n",
    "        \n",
    "        # Convert to Pydantic models\n",
    "        product_models = []\n",
    "        for product in products:\n",
    "            try:\n",
    "                product_models.append(ProductModel(**product))\n",
    "            except Exception as e:\n",
    "                print(f\"Error creating product model: {e}\")\n",
    "                continue\n",
    "        \n",
    "        return ScrapeResponse(\n",
    "            status=\"success\",\n",
    "            count=len(product_models),\n",
    "            products=product_models,\n",
    "            message=f\"Successfully scraped {len(product_models)} products\"\n",
    "        )\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=f\"Scraping failed: {str(e)}\")\n",
    "\n",
    "@app.get(\"/scrape\")\n",
    "async def scrape_products_get(\n",
    "    category: str = Query(..., description=\"Category to scrape (e.g., mobile-cases-covers)\"),\n",
    "    start_page: int = Query(1, description=\"Start page number\"),\n",
    "    end_page: int = Query(1, description=\"End page number\"),\n",
    "    use_selenium: bool = Query(True, description=\"Use Selenium for dynamic content\"),\n",
    "    format: str = Query(\"json\", description=\"Response format: json or csv\")\n",
    "):\n",
    "    \"\"\"GET endpoint for scraping products\"\"\"\n",
    "    global scraper_instance\n",
    "    \n",
    "    try:\n",
    "        if not scraper_instance:\n",
    "            scraper_instance = DarazScraper(use_selenium=use_selenium)\n",
    "        \n",
    "        products = scraper_instance.scrape_category(category, start_page, end_page)\n",
    "        \n",
    "        if format.lower() == \"csv\":\n",
    "            # Return CSV data\n",
    "            df = pd.DataFrame(products)\n",
    "            csv_content = df.to_csv(index=False)\n",
    "            return {\"csv_data\": csv_content, \"count\": len(products)}\n",
    "        \n",
    "        return {\n",
    "            \"status\": \"success\",\n",
    "            \"count\": len(products),\n",
    "            \"products\": products\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "@app.get(\"/categories\")\n",
    "async def get_popular_categories():\n",
    "    \"\"\"Get list of popular categories\"\"\"\n",
    "    return {\n",
    "        \"categories\": [\n",
    "            \"mobile-cases-covers\",\n",
    "            \"smartphones\",\n",
    "            \"laptops\",\n",
    "            \"fashion-womens\",\n",
    "            \"fashion-mens\",\n",
    "            \"electronics\",\n",
    "            \"home-garden\",\n",
    "            \"sports-outdoor\",\n",
    "            \"health-beauty\",\n",
    "            \"baby-toys\"\n",
    "        ]\n",
    "    }\n",
    "\n",
    "@app.get(\"/health\")\n",
    "async def health_check():\n",
    "    \"\"\"Health check endpoint\"\"\"\n",
    "    global scraper_instance\n",
    "    return {\n",
    "        \"status\": \"healthy\",\n",
    "        \"selenium_available\": scraper_instance and scraper_instance.use_selenium,\n",
    "        \"timestamp\": time.time()\n",
    "    }\n",
    "\n",
    "def run_cli():\n",
    "    \"\"\"CLI mode for direct usage\"\"\"\n",
    "    scraper = DarazScraper(use_selenium=True)\n",
    "    \n",
    "    try:\n",
    "        print(\"Daraz Nepal Product Scraper (CLI Mode)\")\n",
    "        print(\"=\" * 40)\n",
    "        \n",
    "        category = input(\"Enter category (e.g., mobile-cases-covers): \").strip()\n",
    "        start_page = int(input(\"Enter start page (default 1): \") or \"1\")\n",
    "        end_page = int(input(\"Enter end page (default 1): \") or \"1\")\n",
    "        \n",
    "        print(f\"\\nScraping {category} from page {start_page} to {end_page}...\")\n",
    "        products = scraper.scrape_category(category, start_page, end_page)\n",
    "        \n",
    "        if products:\n",
    "            df = pd.DataFrame(products)\n",
    "            csv_file = f\"daraz_{category}_p{start_page}-{end_page}.csv\"\n",
    "            df.to_csv(csv_file, index=False)\n",
    "            \n",
    "            print(f\"\\n Successfully scraped {len(products)} products!\")\n",
    "            print(f\" Saved to: {csv_file}\")\n",
    "            print(\"\\n Sample products:\")\n",
    "            print(df[['title', 'price', 'rating']].head(3).to_string(index=False))\n",
    "        else:\n",
    "            print(\" No products were scraped.\")\n",
    "            \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n\\n Scraping interrupted by user\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n Error: {e}\")\n",
    "    finally:\n",
    "        scraper.close_driver()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if len(sys.argv) > 1 and sys.argv[1] == \"--api\":\n",
    "        print(\" Starting FastAPI server...\")\n",
    "        uvicorn.run(app, host=\"0.0.0.0\", port=8000, reload=False)\n",
    "    else:\n",
    "        run_cli()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5659276-78d0-41b5-a22e-77d035bc74b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
